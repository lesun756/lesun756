{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Word Embedding Network\n",
    "In this assignment you will practice how to create a Word Embedding Network in Tensorflow 2.0. First, you will finish some functions to parse the data, build the corpus and construct the skip pair. Then, you will construct a word embedding network by follow the specific requirements and architectures. Finally, you will train the network and visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split_sentence\n",
    "1. Remove the special characters from the sentence\n",
    "2. Filter the short sentence\n",
    "\n",
    "You can rewrite this function or add more filter conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(sentences):\n",
    "    new_sentence = list()\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.replace(\"\\n\", \" \")\n",
    "        sentence = sentence.replace(\",\", \"\")\n",
    "        sentence = sentence.replace(\"\\'\", \" \")\n",
    "        sentence = sentence.replace(\"?\", \"\")\n",
    "        sentence = sentence.replace(\"!\", \"\")\n",
    "        sentence = sentence.replace(\";\", \"\")\n",
    "        sentence = sentence.lower()\n",
    "        if sentence.count(\" \") <= 3:\n",
    "            continue\n",
    "        new_sentence.append(sentence)\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all the sentences from the input file. Split the input into each sentence by calling the \"split_sentence\" function.\n",
    "\n",
    "- test_doc_short: Small dataset. You can use it to debug your code.\n",
    "\n",
    "- test_doc_long: Large dataet. You should use it to get the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"test_doc_long.txt\",'r')\n",
    "raw_data_1 = file.read()\n",
    "file.close()\n",
    "sentences = raw_data_1.split(\".\")\n",
    "print (len(sentences))\n",
    "corpus_raw = list()                  \n",
    "corpus_raw = split_sentence(sentences)\n",
    "print (corpus_raw[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_dictionary (10 points)\n",
    "1. Extract the word from the input. \n",
    "2. Build a non-duplicate word dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(corpus_raw):\n",
    "    words = []\n",
    "    # TO DO\n",
    "\n",
    "    # END TO DO\n",
    "    return set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = build_dictionary(corpus_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of word in \"test_doc_long\" dataset is around 7.\n",
    "- The number of word in \"test_doc_long\" dataset is around 1831."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(corpus_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one_hot_encoding (10 points)\n",
    "1. Every word is represented as a vector containing 1 at its position in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(data_point_index, vocab_size):\n",
    "    # TO DO\n",
    "\n",
    "\n",
    "\n",
    "    # END TO DO\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_word_index_mapping (10 points)\n",
    "1. Given a word, the function should return the index of this word in dictionary.\n",
    "2. Given an index, the function should retrieve the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_index_mapping(corpus_dict):   \n",
    "    # TO DO\n",
    "\n",
    "\n",
    "\n",
    "    # END TO DO\n",
    "    return word_2_ind, ind_2_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_ind, ind_2_word = build_word_index_mapping(corpus_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Output:\n",
    "    \n",
    "1831\n",
    "1831\n",
    "1504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(word_2_ind))\n",
    "print (len(ind_2_word))\n",
    "print (word_2_ind['he'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = corpus_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_skip_pair (10 points)\n",
    "1. Build the word pair with given window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_skip_pair(window_size, sentences):\n",
    "    # TO DO\n",
    "\n",
    "\n",
    "\n",
    "    # END TO DO\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Input:\n",
    "\n",
    "He is the king .\n",
    "\n",
    "Example Output: \n",
    "\n",
    "[['he', 'is'], ['he', 'the'], ['is', 'he'], ['is', 'the'], ['is', 'king']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = build_skip_pair(WINDOW_SIZE, sentences)\n",
    "print (len(data))\n",
    "print (data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_train_data_label\n",
    "1. Iterate all the word pairs in data\n",
    "2. Construct the train and label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_data_label(data, word_2_ind, vocab_size):\n",
    "    x_train = [] \n",
    "    y_train = [] \n",
    "    for data_word in data:\n",
    "        x_train.append(one_hot_encoding(word_2_ind[ data_word[0] ], vocab_size))\n",
    "        y_train.append(one_hot_encoding(word_2_ind[ data_word[1] ], vocab_size))\n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = build_train_data_label(data, word_2_ind, len(corpus_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the dataset with batch size 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "\n",
    "dataset = dataset.shuffle(100).batch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyEmbeddingModel (20 points)\n",
    "1. init: Define all the layers you will use in the embedding network.\n",
    "2. call: Define the network layer connectivity:\n",
    "           - Fully connected with embedding_size/2 hidden neurons\n",
    "           - Batchnormalization (optional)\n",
    "           - Relu activation (optional)\n",
    "           - Fully connected with embedding_size hidden neurons (This should be the word embedding output)\n",
    "           - Batchnormalization (optional)\n",
    "           - Relu activation (optional)\n",
    "           - Fully connected that map to vocab_size output classes\n",
    "           - Softmax (This should be the classification output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEmbeddingModel(Model):\n",
    "  def __init__(self, embedding_size, vocab_size):\n",
    "    super(MyEmbeddingModel, self).__init__()\n",
    "    #Example:\n",
    "        #self.d2 = Dense(embedding_size)\n",
    "        #self.d3 = Dense(vocab_size, activation = 'softmax')\n",
    "    \n",
    "    # TO DO\n",
    "\n",
    "\n",
    "\n",
    "    # END TO DO\n",
    "\n",
    "  def call(self, x):\n",
    "    #Example:\n",
    "        #x_2 = self.d2(x_1)\n",
    "        #x_3 = self.d3(x_2)\n",
    "        \n",
    "    # TO DO\n",
    "\n",
    "\n",
    "\n",
    "    # END TO DO\n",
    "    return x_2, x_3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "vocab_size = len(corpus_dict)\n",
    "model = MyEmbeddingModel(embedding_size, vocab_size)\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer (10 points)\n",
    "- Implement the SGD optimizer\n",
    "- Implement the RMSprop optimizer\n",
    "- Implement the Adagrad optimizer\n",
    "- Implement the Adadelta optimizer\n",
    "- Implement the Adam optimizer (Use the Adam optimizer for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: optimizer = tf.keras.optimizers.Adamax()\n",
    "# optimizer = #SGD optimizer\n",
    "# optimizer = #RMSprop optimizer\n",
    "# optimizer = #Adagrad optimizer\n",
    "# optimizer = #Adadelta optimizer\n",
    "optimizer = #Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training step. Calculate the loss and optimize the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        _, predictions = model(inputs, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss = tf.reduce_mean(loss)\n",
    "\n",
    "    return train_loss, labels, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Output:\n",
    "\n",
    "    Epoch 0, Loss: 6.393890857696533\n",
    "    Epoch 1, Loss: 5.391612529754639\n",
    "    Epoch 2, Loss: 4.996224403381348\n",
    "    Epoch 3, Loss: 4.692948341369629\n",
    "    Epoch 4, Loss: 4.473527908325195\n",
    "    Epoch 5, Loss: 4.335629940032959\n",
    "    Epoch 6, Loss: 4.251341342926025\n",
    "    Epoch 7, Loss: 4.205460071563721\n",
    "    Epoch 8, Loss: 4.172143936157227\n",
    "    Epoch 9, Loss: 4.1499714851379395\n",
    "    Epoch 10, Loss: 4.129685878753662\n",
    "    ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    batch_loss = 0.0\n",
    "    num_batch = 0\n",
    "    for (batch, (inputs, labels)) in enumerate(dataset):\n",
    "        train_loss, labels, predictions = train_step(inputs, labels)\n",
    "        batch_loss += train_loss\n",
    "        num_batch += 1\n",
    "    template = 'Epoch {}, Loss: {}'\n",
    "    print(template.format(epoch, batch_loss/num_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_embedding_dict (10 points)\n",
    "1. Iterate the corpus_dict and generate the embedding for each word.\n",
    "2. Use the trained model to generate the word embedding with given one-hot embedding word.\n",
    "3. Store the word and embedding in a dictionary. The key should be the word. The value should be the embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_dict(model, corpus_dict):\n",
    "    embeddings = dict()\n",
    "    # TO DO\n",
    "\n",
    "    # example (use the trained model to generate the word embedding with given one-hot embedding word): sample_embedding, _ = model(sample_one_hot)\n",
    "\n",
    "    # END TO DO\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# euclidean_dist_np (10 points)\n",
    "1. Calculate the Euclidean distance between two input vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist_np(vec1, vec2):\n",
    "    dist = 0.0\n",
    "    # TO DO\n",
    "\n",
    "\n",
    "\n",
    "    # END TO DO\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find_closest (10 points)\n",
    "1. Calculate the euclidean distance between the given word and all the words in embedding dictionary.\n",
    "2. Sort the dictionary by value in ascending order.\n",
    "3. Return the first three closet words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(word, embeddings):\n",
    "    result = dict()\n",
    "    # TO DO\n",
    "\n",
    "\n",
    "\n",
    "    # END TO DO\n",
    "    return result_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Output: \n",
    "\n",
    "[('she', 0.0), ('he', 5.3993783), ('they', 5.7223315)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = build_embedding_dict(model, corpus_dict)\n",
    "print(find_closest('she', embedding_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize_cluster\n",
    "1. Visualize the word embedding in 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cluster(embedding_dict): \n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for w in embedding_dict.keys():\n",
    "        labels.append(w)\n",
    "        tokens.append(embedding_dict[w])\n",
    "    tsne_model = TSNE(perplexity=10, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "\n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                         xy=(x[i], y[i]),\n",
    "                         xytext=(5, 2),\n",
    "                         textcoords='offset points',\n",
    "                         ha='right',\n",
    "                         va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cluster(embedding_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
